{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60975bde",
   "metadata": {},
   "source": [
    "# RAG API (Kaggle) — FastAPI + ngrok + Hugging Face local model\n",
    "\n",
    "**What this does:**  \n",
    "- Ingest a user-supplied PDF → split into chunks → embed → build FAISS index  \n",
    "- Serve two endpoints:  \n",
    "  - `POST /ingest` — upload/replace a PDF index  \n",
    "  - `POST /ask` — ask a question; does Retrieval-Augmented Generation with your HF model  \n",
    "- Expose the API publicly via ngrok (copy the URL into your Flutter app)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc7278",
   "metadata": {},
   "source": [
    "## 1) Setup & installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Kaggle, these installs are okay (internet must be on).\n",
    "# You can re-run if you see any transient errors.\n",
    "!pip -q install fastapi==0.111.0 uvicorn==0.30.1 \"python-multipart>=0.0.9\"                pypdf==4.3.1 faiss-cpu==1.8.0.post1                sentence-transformers==3.0.1                transformers==4.43.3 accelerate==0.33.0 bitsandbytes==0.43.3                langchain==0.2.6                pyngrok==7.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2742503",
   "metadata": {},
   "source": [
    "## 2) Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ======== REQUIRED: set these ========\n",
    "# Hugging Face token if your model is gated/private\n",
    "os.environ.setdefault(\"HF_TOKEN\", \"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\")\n",
    "\n",
    "# Your Hugging Face model id (local model), e.g. \"HatemAhmed44/Egiptura_AI\"\n",
    "os.environ.setdefault(\"MODEL_ID\", \"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "\n",
    "# Ngrok authtoken\n",
    "os.environ.setdefault(\"NGROK_AUTHTOKEN\", \"32JYankevjVACV9Gy2F0KQ2YhJ8_6Fne9bccQciyPpEtcJAKR\")\n",
    "\n",
    "# Server port (match Flutter if you hardcode)\n",
    "os.environ.setdefault(\"PORT\", \"8000\")\n",
    "\n",
    "# Embedding model (sentence-transformers)\n",
    "os.environ.setdefault(\"EMBEDDINGS_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Storage paths\n",
    "os.environ.setdefault(\"INDEX_DIR\", \"/kaggle/working/rag_index\")\n",
    "os.environ.setdefault(\"UPLOADS_DIR\", \"/kaggle/working/uploads\")\n",
    "os.environ.setdefault(\"CACHE_DIR\", \"/kaggle/working/hf_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08edfc",
   "metadata": {},
   "source": [
    "## 3) Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, json, time, threading, uuid\n",
    "from typing import Optional, List\n",
    "\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Embeddings / FAISS\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# HF text-generation\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    "\n",
    "# ngrok\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Ensure dirs\n",
    "INDEX_DIR = os.getenv(\"INDEX_DIR\")\n",
    "UPLOADS_DIR = os.getenv(\"UPLOADS_DIR\")\n",
    "CACHE_DIR = os.getenv(\"CACHE_DIR\")\n",
    "for d in (INDEX_DIR, UPLOADS_DIR, CACHE_DIR):\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 120) -> List[str]:\n",
    "    '''\n",
    "    Simple fixed-size chunking to avoid heavy dependencies.\n",
    "    '''\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(len(tokens), start + chunk_size)\n",
    "        chunk = \" \".join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "        if (chunk_size - overlap) <= 0:\n",
    "            break\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a4932",
   "metadata": {},
   "source": [
    "## 4) Load embeddings + build/load FAISS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67faff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL_ID = os.getenv(\"EMBEDDINGS_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "_embedder = SentenceTransformer(EMBED_MODEL_ID)\n",
    "\n",
    "def _index_paths():\n",
    "    return (os.path.join(INDEX_DIR, \"faiss.index\"),\n",
    "            os.path.join(INDEX_DIR, \"meta.json\"))\n",
    "\n",
    "def has_index() -> bool:\n",
    "    idx_path, meta_path = _index_paths()\n",
    "    return os.path.exists(idx_path) and os.path.exists(meta_path)\n",
    "\n",
    "def reset_index_dir():\n",
    "    if os.path.exists(INDEX_DIR):\n",
    "        shutil.rmtree(INDEX_DIR, ignore_errors=True)\n",
    "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "\n",
    "def build_index_from_chunks(chunks: List[str]):\n",
    "    reset_index_dir()\n",
    "    idx_path, meta_path = _index_paths()\n",
    "    # Embed\n",
    "    embs = _embedder.encode(chunks, batch_size=64, convert_to_numpy=True, show_progress_bar=True)\n",
    "    dim = embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    # Normalize for cosine\n",
    "    faiss.normalize_L2(embs)\n",
    "    index.add(embs)\n",
    "    faiss.write_index(index, idx_path)\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"chunks\": chunks}, f, ensure_ascii=False)\n",
    "\n",
    "def load_index():\n",
    "    idx_path, meta_path = _index_paths()\n",
    "    index = faiss.read_index(idx_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    return index, meta[\"chunks\"]\n",
    "\n",
    "def search_chunks(query: str, k: int = 5):\n",
    "    if not has_index():\n",
    "        return []\n",
    "    index, chunks = load_index()\n",
    "    q_emb = _embedder.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    I = I[0].tolist()\n",
    "    top_chunks = [chunks[i] for i in I if i < len(chunks)]\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efce64",
   "metadata": {},
   "source": [
    "## 5) Load local HF model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\", \"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "\n",
    "# Use a standard causal LM interface\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ")\n",
    "\n",
    "def generate_answer(prompt: str, max_new_tokens: int = 256, temperature: float = 0.2, top_p: float = 0.95):\n",
    "    input_ids = tok.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tok.eos_token_id\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    # Return only the newly generated continuation\n",
    "    return text[len(tok.decode(input_ids[0], skip_special_tokens=True)) : ].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b8154",
   "metadata": {},
   "source": [
    "## 6) FastAPI app + endpoints `/ingest` and `/ask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "app = FastAPI(title=\"Egiptura RAG API\", version=\"1.0\")\n",
    "\n",
    "# CORS: allow all for easy Flutter dev; lock down later if needed\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class AskRequest(BaseModel):\n",
    "    question: str\n",
    "    lang: Optional[str] = None  # \"es\", \"ar\", \"en\" ... optional\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"ok\": True, \"has_index\": has_index()}\n",
    "\n",
    "@app.post(\"/ingest\")\n",
    "async def ingest_pdf(file: UploadFile = File(...)):\n",
    "    # Save uploaded PDF\n",
    "    file_id = str(uuid.uuid4())\n",
    "    save_path = os.path.join(UPLOADS_DIR, f\"{file_id}.pdf\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(await file.read())\n",
    "\n",
    "    # Extract text\n",
    "    reader = PdfReader(save_path)\n",
    "    pages = []\n",
    "    for p in reader.pages:\n",
    "        try:\n",
    "            pages.append(p.extract_text() or \"\")\n",
    "        except Exception:\n",
    "            pages.append(\"\")\n",
    "    full_text = \"\\n\".join(pages)\n",
    "\n",
    "    if not full_text.strip():\n",
    "        raise HTTPException(status_code=400, detail=\"PDF text is empty or could not be extracted.\")\n",
    "\n",
    "    # Chunk + index\n",
    "    chunks = chunk_text(full_text, chunk_size=180, overlap=40)\n",
    "    if len(chunks) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"No chunks could be created from the PDF.\")\n",
    "\n",
    "    build_index_from_chunks(chunks)\n",
    "    return {\"ok\": True, \"chunks\": len(chunks)}\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask(req: AskRequest):\n",
    "    q = (req.question or \"\").strip()\n",
    "    if not q:\n",
    "        raise HTTPException(status_code=400, detail=\"Empty question\")\n",
    "\n",
    "    # Retrieve\n",
    "    ctx_chunks = search_chunks(q, k=6)\n",
    "\n",
    "    # Build prompt\n",
    "    context_block = \"\\n\\n\".join([f\"- {c}\" for c in ctx_chunks])\n",
    "    system_rules = (\n",
    "        \"You are Egiptura's travel assistant. Answer ONLY from the provided context. \"\n",
    "        \"If the answer is not in the context, say you don't have enough information.\"\n",
    "    )\n",
    "    prompt = f\"\"\"{system_rules}\n",
    "\n",
    "Context:\n",
    "{context_block}\n",
    "\n",
    "Question: {q}\n",
    "Answer in the same language as the question, concise and well-structured.\n",
    "\"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt, max_new_tokens=256, temperature=0.2, top_p=0.95)\n",
    "    return {\"answer\": answer, \"used_chunks\": len(ctx_chunks)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc291a",
   "metadata": {},
   "source": [
    "## 7) Launch server + ngrok tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn, os\n",
    "from threading import Thread\n",
    "\n",
    "PORT = int(os.getenv(\"PORT\", \"8000\"))\n",
    "NGROK_AUTHTOKEN = os.getenv(\"NGROK_AUTHTOKEN\")\n",
    "\n",
    "# Start uvicorn in a thread\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
    "\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Start ngrok\n",
    "if NGROK_AUTHTOKEN and NGROK_AUTHTOKEN != \"32JYankevjVACV9Gy2F0KQ2YhJ8_6Fne9bccQciyPpEtcJAKR\":\n",
    "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
    "public_url = ngrok.connect(addr=PORT, proto=\"http\").public_url\n",
    "public_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c76315",
   "metadata": {},
   "source": [
    "## 8) Quick test cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 Health check (after server is up)\n",
    "import requests, time\n",
    "time.sleep(2)\n",
    "try:\n",
    "    r = requests.get(f\"{public_url}/health\", timeout=10)\n",
    "    r.json()\n",
    "except Exception as e:\n",
    "    print(\"Health check failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071a1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Example: Ingest a PDF programmatically (if you have a local path on Kaggle)\n",
    "# Replace 'path_to_pdf' with your file path in Kaggle environment.\n",
    "# import requests\n",
    "# files = {\"file\": open(\"/kaggle/input/some.pdf\", \"rb\")}\n",
    "# r = requests.post(f\"{public_url}/ingest\", files=files)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Ask a question\n",
    "# Replace with your own question once the index is built.\n",
    "# data = {\"question\": \"¿Qué incluye el programa de crucero por el Nilo?\"}\n",
    "# r = requests.post(f\"{public_url}/ask\", json=data)\n",
    "# r.json()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
